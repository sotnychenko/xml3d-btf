\chapter{BTF compression methods}
\label{chapter:compression_methods}



 BTF data consists of thousands of images, which means single BTF requires lots of storage space.
Bonn  Database \cite{btfBonn} consists of 8-bit PNG images with a resolution of $256\times256$ sampled for $81\times81$ different camera and light directions.
 The uncompressed data occupies approx. 1,2 GB of space. And that is only for one BTF material.
 To render the scene with several BTFs and to achieve acceptable frame-rates becomes practically impossible task, especially for low-end hardware.
 Also, as we intent to render BTF in a web-browser, BTF data would have to be transferred from a server to a client, which means the compact representation of BTF is inevitable in such case.
 For our scenario it is important to chose the right compression method, i.e. the one that can allows real-time decompression, high compression rate while preserving good quality, and separability of compressed BTF data.
 Separability is needed for real-time streaming in a web-browser.  
 As the data is transferred in small chunks during the stream and at some point the rendering has to be refreshed, thus it is inevitable to have the ability to decompress full BTF sample having only partial data.

There are different types of methods applied for compressing the BTF. Those methods can be categorized the following way:

\begin{itemize}
  \item \emph{Analytic methods} group, where BTF is represented by analytic BRDF models. 
Analytic BRDF models are the functions which fit separately each texel of BTF. Such functions store few parameters, thus high real-time performance is easily achieved.
However, these group of methods can suffer from decreased quality \cite{haindl}. Also, it is hard to change parameters in order to control the visual quality. Chapter  \ref{section:analytic_methods}.
   \item \emph{Statistic methods}, which belong to methods that reduce dimensionality based on  statistics, for instance based linear basis decompression method such as PCA (Principal component analysis). 
    PCA based methods are frequently used, because its parameters directly correspond to the trade-off between compression ratio and reconstruction quality.
    Also, PCA is frequently a basis point for some more sophisticated methods \cite{webglbtfstreaming}. Chapter \ref{section:stat_methods}.
   \item \emph{Probabilistic models}, which can spatially enlarge BTF to any arbitrary size without visible discontinuities and extremely compress the original BTF.
   However, the resulted quality usually suits for flat surfaces and there are problems of implementing in GPU for real-time rendering. Chapter \ref{section:prob_methods}.
 \end{itemize}



 \section{Analytic methods}
\label{section:analytic_methods}	
 This group of methods take advantage of ABRDF representation of BTF. 
 There is a large number of techniques which allow compactly represent BRDF, which in essence can be also applied for ABRDF.
 Each texel of BTF, i.e. spatial position of surface are represented as ABRDF. Each of these ABRDFs can be modeled and compressed by any BRDF model.
 
One of the possible ways to model ABRDF is to use \emph{Polynomial Texture  Mapping} (PTM) approach.
Malzbender et. al. \cite{PTM} used this approach which allows for high compression rates and generally good quality. 
However, PTM requires to compute specular and diffuse effects separately. 
PTM model assumes that the input surfaces has to be either diffuse or their specular contribution is separated beforehand.
For BTF it can be quite difficult to separate specular highlights \cite{haindl}.

Haindl et. al. \cite{haindl} applied PTM for a fixed camera positions of BTF, i.e. for reflectance fields (RF).
General formula looks the following way (PTM RF):

{\centering$R_{o}(r,i)\approx a_{0}(r)u_{x}^2+a_{1}(r)u_{y}^2+a_{2}(r)u_{x}u_{y}+a_{3}(r)u_{x}+a_{4}(r)u_{y}+a_{5}(r)$\\}

 where $R_{o}$ is approximated RF for fixed camera direction $o$ and $u_{x},u_{y}$ are projections of the normalized light vector into the local coordinate system $r=(x,y)$.
 Set of all possible $R_{o}$ is the number of all camera positions, i.e. $n_{o}$. 
 Coefficients $a_{p}$ are fitted by the use of \emph{singular value decomposition} SVD for each $R_{o}$ and parameters stored as a spatial map referred to as a PTM.
 
 However, Malzbender et. al. \cite{PTM} claims that this method produce considerable errors for high grazing angles. 
 But, nevertheless this method enables fast rendering and generally suited for smooth material surfaces.
 
 Another model which produces slightly better visual quality is the polynomial extension of one-lobe Lafortune model (PLM).
One-lobe Lafortune model (LM) looks the following way \cite{plm}:

{\centering$Y_{o} (r,i) = \rho_{o,r}(C_{o,r,x}u_{x}+C_{o,r,y}u_{y}+C_{o,r,z}u_{z})_{ }^{n_{o,r}}$\\}

where $w_{i}(\theta_{i}, \phi_{i})=[u_{1},u_{2},u_{3}]^{T}$ is a unit vector pointing to light position.
Parameters $\rho,C_{x},C_{y},C_{z},n$ can be computed with a Levenberg-Marquardt non-linear optimisation algorithm \cite{plm}. 
During testing of this model Filip and Haindl \cite{plm} claim that LM produce unsatisfactory results for complex ABRDFs.
 Thus, the polynomial extension of one-lobe Lafortune was introduced (PLM RF):

{\centering$R_{o}(r,i)\approx  \sum_{j=0}^{n} a_{r,o,i,j}Y_{o}(r,i)^j$\\}



PLM RF solves the problem of bad quality for grazing angles and improves the rendering quality compared to PTM RF.
However, statistic based methods produce even better quality compared to above methods but with lower compression rates.



  \section{Statistic methods}
\label{section:stat_methods}


 \section{Probabilistic models}
\label{section:prob_methods}